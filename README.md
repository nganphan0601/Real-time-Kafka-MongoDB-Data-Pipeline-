# Real-time-Kafka-MongoDB-Data-Pipeline-
This project demonstrates the implementation of a real-time streaming data pipeline using Apache Kafka, Python, and MongoDB.
In modern data-driven applications, real-time processing is essential for monitoring, analytics, and instant decision-making. This project simulates a real-world scenario where streaming data is ingested from one source, transformed, and stored for further analysis.

<h2>Features:</h2>

Consumes data from a given Kafka source.

Produces processed data into a custom Kafka topic.

Reads data from the created topic and stores it in MongoDB.

Implements Python-based producers and consumers for Kafka.

<h2>Tech Stack & Skills Gained:</h2>

Apache Kafka: topic creation, producerâ€“consumer architecture, partitioning.

Python: data serialization/deserialization (JSON), Kafka client programming.

MongoDB: NoSQL data storage integration.

Real-time data streaming concepts: message brokers, event-driven architecture.

System integration: connecting multiple technologies into a cohesive data pipeline.
